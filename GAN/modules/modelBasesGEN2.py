import itertools

import tensorflow as tf
from keras import backend as K
from tensorflow.keras import layers
from tensorflow.python.framework import ops
from keras.utils.generic_utils import get_custom_objects
from GAN.modules.modelBases import upsample_block, conv_block

noise_dim = 32
DATA_INPUT = (158, 1)


def binary(x):
    return K.relu(K.sign(x))

# def binarize(x):
#     """
#     Clip and binarize tensor using the straight through estimator (STE) for the gradient.
#     """
#     g = tf.get_default_graph()
#
#     with ops.name_scope("Binarized") as name:
#         with g.gradient_override_map({"Sign": "Identity"}):
#             return tf.sign(x)


def make_activator(activations):
    def activator(t):
        slices = tf.unstack(t, axis=1)
        activated = []
        for s, act in zip(slices, itertools.cycle(activations)):
            activated.append(act(s))
        return tf.stack(activated)
    return activator


# def HardTanh(name='HardTanh'):
#     def layer(x, is_training=True):
#         with tf.variable_scope(name, None, [x]):
#             return tf.clip_by_value(x, -1, 1)
#
#     return layer


class GeneratorBaseGEN2:
    @staticmethod
    def get_generator_model():
        get_custom_objects().update({'binary': layers.Activation(binary)})

        noise = layers.Input(shape=(noise_dim,))

        l1 = layers.Dense(2 * 5, use_bias=False)(noise)
        l1 = layers.BatchNormalization()(l1)
        l1 = layers.LeakyReLU(0.2)(l1)
        l1 = layers.Reshape((2, 5))(l1)

        l1 = upsample_block(
            l1,
            2,
            layers.LeakyReLU(0.2),
            strides=4,
            use_bias=False,
            use_bn=True,
            padding="same",
            use_dropout=False,
        )
        l1 = layers.Flatten()(l1)
        l1 = layers.Activation('sigmoid')(l1)
        # l1 = layers.Dense(2,activation='relu')(l1)

        l2 = layers.Dense(13 * 10, use_bias=False)(noise)
        l2 = layers.BatchNormalization()(l2)
        l2 = layers.LeakyReLU(0.2)(l2)

        l2 = layers.Reshape((13, 10))(l2)
        l2 = upsample_block(
            l2,
            20,
            layers.LeakyReLU(0.2),
            strides=(2),
            # up_size=4,
            use_bias=False,
            use_bn=True,
            padding="same",
            use_dropout=False,
        )
        l2 = upsample_block(
            l2,
            12,
            layers.LeakyReLU(0.2),
            strides=(2),
            # up_size=4,
            use_bias=False,
            use_bn=True,
            padding="same",
            use_dropout=False,
        )

        l2 = layers.Flatten()(l2)
        l2 = layers.Activation('binary')(l2)

        x = layers.Concatenate(axis=1)([l1, l2])

        # At this point, we have an output which has the same shape as the input, (32, 32, 1).
        # We will use a Cropping2D layer to make it (28, 28, 1).
        # x = layers.Cropping1D(1)(x)

        x = layers.Reshape((158,1))(x)
        g_model = tf.keras.models.Model(noise, x, name="generator")
        return g_model

    def forward(self, x):
        x = self.model(x)
        return (
            x if self.final_activation is None else self.final_activation(x)
        )


class DiscriminatorBaseGEN2:
    @staticmethod
    def get_discriminator_model():
        data_input = layers.Input(shape=DATA_INPUT)
        # Zero pad the input to make the input images size to (32, 32, 1).
        # x = layers.ZeroPadding1D(1)(data_input)
        l1, l2 = layers.Lambda(lambda m: tf.split(m, (2, 156), 1))(data_input)

        l1 = conv_block(
            l1,
            16,
            kernel_size=5,
            strides=2,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
        )
        l1 = conv_block(
            l1,
            8,
            kernel_size=5,
            strides=2,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
        )

        l2 = conv_block(
            l2,
            32,
            kernel_size=5,
            strides=2,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
        )
        l2 = conv_block(
            l2,
            16,
            kernel_size=5,
            strides=3,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
        )
        l2 = conv_block(
            l2,
            8,
            kernel_size=5,
            strides=4,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
        )

        x = layers.Concatenate(axis=1)([l1, l2])

        x = layers.Flatten()(x)
        x = layers.Dropout(0.3)(x)
        x = layers.Dense(1)(x)

        d_model = tf.keras.models.Model(data_input, x, name="discriminator")
        return d_model
