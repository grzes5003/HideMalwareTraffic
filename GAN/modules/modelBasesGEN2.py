import itertools

import tensorflow as tf
from keras import backend as K
from tensorflow.keras import layers
from tensorflow.python.framework import ops
from keras.utils.generic_utils import get_custom_objects
from GAN.modules.modelBases import upsample_block, conv_block

# noise_dim = 32
# noise_dim = 158
# noise_dim = 160
noise_dim = 12
# DATA_INPUT = (158, 1)
DATA_INPUT = (12, 1)


def binary(x):
    return K.relu(K.sign(x))

# def binarize(x):
#     """
#     Clip and binarize tensor using the straight through estimator (STE) for the gradient.
#     """
#     g = tf.get_default_graph()
#
#     with ops.name_scope("Binarized") as name:
#         with g.gradient_override_map({"Sign": "Identity"}):
#             return tf.sign(x)


def make_activator(activations):
    def activator(t):
        slices = tf.unstack(t, axis=1)
        activated = []
        for s, act in zip(slices, itertools.cycle(activations)):
            activated.append(act(s))
        return tf.stack(activated)
    return activator


# def HardTanh(name='HardTanh'):
#     def layer(x, is_training=True):
#         with tf.variable_scope(name, None, [x]):
#             return tf.clip_by_value(x, -1, 1)
#
#     return layer


class GeneratorBaseGEN2:
    @staticmethod
    def get_generator_model():
        get_custom_objects().update({'binary': layers.Activation(binary)})

        noise = layers.Input(shape=(noise_dim,))

        l1 = layers.Dense(2 * 5, use_bias=False)(noise)
        l1 = layers.BatchNormalization()(l1)
        l1 = layers.LeakyReLU(0.2)(l1)
        l1 = layers.Reshape((2, 5))(l1)

        l1 = upsample_block(
            l1,
            2,
            layers.LeakyReLU(0.2),
            strides=4,
            use_bias=False,
            use_bn=True,
            padding="same",
            use_dropout=False,
        )
        l1 = layers.Flatten()(l1)
        l1 = layers.Activation('sigmoid')(l1)
        # l1 = layers.Dense(2,activation='relu')(l1)

        l2 = layers.Dense(13 * 10, use_bias=False)(noise)
        l2 = layers.BatchNormalization()(l2)
        l2 = layers.LeakyReLU(0.2)(l2)

        l2 = layers.Reshape((13, 10))(l2)
        l2 = upsample_block(
            l2,
            20,
            layers.LeakyReLU(0.2),
            strides=(2),
            # up_size=4,
            use_bias=False,
            use_bn=True,
            padding="same",
            use_dropout=False,
        )
        l2 = upsample_block(
            l2,
            12,
            layers.LeakyReLU(0.2),
            strides=(2),
            # up_size=4,
            use_bias=False,
            use_bn=True,
            padding="same",
            use_dropout=False,
        )

        l2 = layers.Flatten()(l2)
        l2 = layers.Activation('sigmoid')(l2)

        x = layers.Concatenate(axis=1)([l1, l2])

        # At this point, we have an output which has the same shape as the input, (32, 32, 1).
        # We will use a Cropping2D layer to make it (28, 28, 1).
        # x = layers.Cropping1D(1)(x)

        x = layers.Reshape((158,1))(x)
        g_model = tf.keras.models.Model(noise, x, name="generator")
        return g_model

    @staticmethod
    def get_generator_model_full():
        noise = layers.Input(shape=(noise_dim,))

        x = layers.Dense(158)(noise)
        x = layers.BatchNormalization()(x)
        x = layers.LeakyReLU(0.2)(x)

        x = layers.Dense(158)(noise)
        x = layers.BatchNormalization()(x)
        x = layers.LeakyReLU(0.2)(x)

        x = layers.Dense(158, use_bias=False)(noise)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('sigmoid')(x)

        x = layers.Reshape((158, 1))(x)

        # At this point, we have an output which has the same shape as the input, (32, 32, 1).
        # We will use a Cropping2D layer to make it (28, 28, 1).
        # x = layers.Cropping1D((2))(x)

        g_model = tf.keras.models.Model(noise, x, name="generator")
        return g_model

    @staticmethod
    def get_generator_model_conv_full():
        noise = layers.Input(shape=(noise_dim,))

        x = layers.Reshape((158, 1))(noise)
        x = layers.BatchNormalization()(x)
        x = layers.LeakyReLU(0.2)(x)

        x = upsample_block(
            x,
            32,
            layers.LeakyReLU(0.2),
            up_size=1,
            use_bias=False,
            use_bn=True,
            use_dropout=False,
        )

        x = upsample_block(
            x,
            16,
            layers.LeakyReLU(0.2),
            up_size=1,
            use_bias=False,
            use_bn=True,
            use_dropout=False,
        )

        x = upsample_block(
            x,
            1,
            layers.Activation('sigmoid'),
            up_size=1,
            use_bias=False,
            use_bn=True,
            use_dropout=False,
        )

        x = layers.Reshape((158, 1))(x)

        # At this point, we have an output which has the same shape as the input, (32, 32, 1).
        # We will use a Cropping2D layer to make it (28, 28, 1).
        # x = layers.Cropping1D((2))(x)

        g_model = tf.keras.models.Model(noise, x, name="generator")
        return g_model

    @staticmethod
    def get_generator_model_conv_full_v2():
        noise = layers.Input(shape=(160,))

        x = layers.Reshape((4, 40))(noise)
        x = layers.BatchNormalization()(x)
        x = layers.LeakyReLU(0.2)(x)

        x = upsample_block(
            x,
            40,
            layers.LeakyReLU(0.2),
            # strides=10,
            up_size=2,
            use_bias=False,
            use_bn=True,
            use_dropout=False,
        )

        x = upsample_block(
            x,
            20,
            layers.LeakyReLU(0.2),
            up_size=2,
            use_bias=False,
            use_bn=True,
            use_dropout=False,
        )

        x = upsample_block(
            x,
            10,
            layers.Activation('sigmoid'),
            up_size=1,
            use_bias=False,
            use_bn=True,
            use_dropout=False,
        )

        x = layers.Reshape((160,1))(x)

        # At this point, we have an output which has the same shape as the input, (32, 32, 1).
        # We will use a Cropping2D layer to make it (28, 28, 1).
        x = layers.Cropping1D(1)(x)

        g_model = tf.keras.models.Model(noise, x, name="generator")
        return g_model

    @staticmethod
    def get_generator_model_cont():
        noise = layers.Input(shape=(12,))

        x = layers.Dense(144, use_bias=False)(noise)
        x = layers.Reshape((4, 36))(x)
        x = layers.BatchNormalization()(x)
        x = layers.LeakyReLU(0.2)(x)

        x = upsample_block(
            x,
            36,
            layers.LeakyReLU(0.2),
            # strides=10,
            up_size=1,
            # strides=1,
            use_bias=False,
            use_bn=True,
            use_dropout=False,
        )

        x = upsample_block(
            x,
            18,
            layers.LeakyReLU(0.2),
            up_size=1,
            strides=1,
            use_bias=False,
            use_bn=True,
            use_dropout=False,
        )

        x = upsample_block(
            x,
            3,
            layers.Activation('sigmoid'),
            up_size=1,
            use_bias=False,
            use_bn=True,
            use_dropout=False,
        )

        x = layers.Reshape((12, 1))(x)

        # At this point, we have an output which has the same shape as the input, (32, 32, 1).
        # We will use a Cropping2D layer to make it (28, 28, 1).
        # x = layers.Cropping1D(1)(x)

        g_model = tf.keras.models.Model(noise, x, name="generator")
        return g_model

    @staticmethod
    def get_generator_model_cont_dense():
        noise = layers.Input(shape=(noise_dim,))

        x = layers.BatchNormalization()(noise)
        x = layers.LeakyReLU(0.1)(x)
        x = layers.Dense(24)(x)
        x = layers.LeakyReLU(0.15)(x)
        x = layers.Dense(24)(x)
        x = layers.LeakyReLU(0.15)(x)
        x = layers.Dense(DATA_INPUT[0])(x)
        x = layers.Reshape((DATA_INPUT[0], 1))(x)

        # At this point, we have an output which has the same shape as the input, (32, 32, 1).
        # We will use a Cropping2D layer to make it (28, 28, 1).
        # x = layers.Cropping1D(0)(x)

        g_model = tf.keras.models.Model(noise, x, name="generator")
        return g_model

    def forward(self, x):
        x = self.model(x)
        return (
            x if self.final_activation is None else self.final_activation(x)
        )


class DiscriminatorBaseGEN2:
    @staticmethod
    def get_discriminator_model():
        data_input = layers.Input(shape=DATA_INPUT)
        # Zero pad the input to make the input images size to (32, 32, 1).
        # x = layers.ZeroPadding1D(1)(data_input)
        l1, l2 = layers.Lambda(lambda m: tf.split(m, (2, 156), 1))(data_input)

        l1 = conv_block(
            l1,
            16,
            kernel_size=5,
            strides=2,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
        )
        l1 = conv_block(
            l1,
            8,
            kernel_size=5,
            strides=2,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
        )

        l2 = conv_block(
            l2,
            32,
            kernel_size=5,
            strides=2,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
        )
        l2 = conv_block(
            l2,
            16,
            kernel_size=5,
            strides=3,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
        )
        l2 = conv_block(
            l2,
            8,
            kernel_size=5,
            strides=4,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
        )

        x = layers.Concatenate(axis=1)([l1, l2])

        x = layers.Flatten()(x)
        x = layers.Dropout(0.3)(x)
        x = layers.Dense(1)(x)
        # x = layers.Activation('softmax')(x)

        d_model = tf.keras.models.Model(data_input, x, name="discriminator")
        return d_model

    @staticmethod
    def get_discriminator_model_v2():
        data_input = layers.Input(shape=DATA_INPUT)
        x = layers.ZeroPadding1D((0))(data_input)
        x = conv_block(
            x,
            64,
            kernel_size=5,
            strides=2,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
        )
        x = conv_block(
            x,
            32,
            kernel_size=5,
            strides=2,
            use_bn=False,
            activation=layers.LeakyReLU(0.2),
            use_bias=True,
            use_dropout=True,
            drop_value=0.3,
        )
        x = conv_block(
            x,
            16,
            kernel_size=5,
            strides=2,
            use_bn=False,
            activation=layers.LeakyReLU(0.2),
            use_bias=True,
            use_dropout=True,
            drop_value=0.3,
        )
        x = conv_block(
            x,
            8,
            kernel_size=5,
            strides=2,
            use_bn=False,
            activation=layers.LeakyReLU(0.2),
            use_bias=True,
            use_dropout=False,
            drop_value=0.3,
        )

        x = layers.Flatten()(x)
        x = layers.Dropout(0.25)(x)
        x = layers.Dense(1)(x)

        d_model = tf.keras.models.Model(data_input, x, name="discriminator")
        return d_model

    @staticmethod
    def get_discriminator_model_cont():
        data_input = layers.Input(shape=DATA_INPUT)
        x = layers.Dense(32, use_bias=False)(data_input)
        x = conv_block(
            x,
            16,
            kernel_size=2,
            strides=2,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
        )
        x = conv_block(
            x,
            8,
            kernel_size=2,
            strides=2,
            use_bn=False,
            activation=layers.LeakyReLU(0.2),
            use_bias=True,
            use_dropout=True,
            drop_value=0.3,
        )
        x = conv_block(
            x,
            4,
            kernel_size=2,
            strides=1,
            use_bn=False,
            activation=layers.Activation('softmax'),
            use_bias=True,
            use_dropout=True,
            drop_value=0.3,
        )

        x = layers.Flatten()(x)
        x = layers.Dropout(0.25)(x)
        x = layers.Dense(1, activation='softmax')(x)

        d_model = tf.keras.models.Model(data_input, x, name="discriminator")
        return d_model

    @staticmethod
    def get_discriminator_model_dense():
        data_input = layers.Input(shape=DATA_INPUT)

        x = layers.BatchNormalization()(data_input)
        x = layers.Dense(12)(x)
        x = layers.LeakyReLU(0.2)(x)
        x = layers.Dense(6)(x)
        x = layers.Dropout(0.25)(x)
        x = layers.Dense(1)(x)

        d_model = tf.keras.models.Model(data_input, x, name="discriminator_dense")
        return d_model
