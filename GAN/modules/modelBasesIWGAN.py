import itertools

import tensorflow as tf
from keras import backend as K
from tensorflow.keras import layers
from tensorflow.python.framework import ops
from keras.utils.generic_utils import get_custom_objects


noise_dim = 26
DATA_INPUT1 = (2, 1)
DATA_INPUT2 = (156, 1)


def conv_block(
    x,
    filters,
    activation,
    kernel_size=(3),
    strides=(1),
    padding="same",
    use_bias=True,
    use_bn=False,
    use_dropout=False,
    drop_value=0.5,
    name=None
):
    if name is not None:
        x = layers.Conv1D(
            filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias, name=name
        )(x)
    else:
        x = layers.Conv1D(
            filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias
        )(x)
    if use_bn:
        x = layers.BatchNormalization()(x)
    x = activation(x)
    if use_dropout:
        x = layers.Dropout(drop_value)(x)
    return x


def upsample_block(
    x,
    filters,
    activation,
    kernel_size=(3),
    strides=(1),
    up_size=(2),
    padding="same",
    use_bn=False,
    use_bias=True,
    use_dropout=False,
    drop_value=0.3,
    name=None
):
    if name is not None:
        x = layers.UpSampling1D(up_size, name=f'{name}_upsampl')(x)
        x = layers.Conv1D(
            filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias, name=f'{name}_conv'
        )(x)
    else:
        x = layers.UpSampling1D(up_size)(x)
        x = layers.Conv1D(
            filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias
        )(x)

    if use_bn:
        x = layers.BatchNormalization()(x)

    if activation:
        x = activation(x)
    if use_dropout:
        x = layers.Dropout(drop_value)(x)
    return x


def upsample_block_v2(
    x,
    filters,
    activation,
    kernel_size=3,
    strides=1,
    padding="same",
    use_bn=False,
    use_bias=True,
    use_dropout=False,
    drop_value=0.3,
    name=None
):
    if name is not None:
        x = layers.Conv1DTranspose(filters=filters,
                                   kernel_size=kernel_size,
                                   strides=strides,
                                   padding=padding,
                                   use_bias=use_bias,
                                   name=name
                                   )(x)
    else:
        x = layers.Conv1DTranspose(filters=filters,
                                   kernel_size=kernel_size,
                                   strides=strides,
                                   padding=padding,
                                   use_bias=use_bias,
                                   )(x)

    if use_bn:
        x = layers.BatchNormalization()(x)

    if activation:
        x = activation(x)
    if use_dropout:
        x = layers.Dropout(drop_value)(x)
    return x


class GeneratorBaseIWGAN:
    @staticmethod
    def get_generator_model():
        noise = layers.Input(shape=(noise_dim,))

        l1 = layers.Dense(2 * 5, use_bias=False, name='l1_dense1')(noise)
        l1 = layers.BatchNormalization()(l1)
        l1 = layers.LeakyReLU(0.25)(l1)

        l1 = layers.Dense(20, name='l1_dense2')(l1)
        l1 = layers.LeakyReLU(0.1)(l1)
        l1 = layers.Dense(30, name='l1_dense3')(l1)
        l1 = layers.LeakyReLU(0.1)(l1)
        l1 = layers.Dense(2, name='l1_dense4')(l1)
        l1 = layers.LeakyReLU(0.1)(l1)
        l1 = layers.Activation('sigmoid')(l1)
        l1 = layers.Reshape(DATA_INPUT1)(l1)

        l2 = layers.Dense(13 * 2, use_bias=False, name='l2_dense1')(noise)
        l2 = layers.BatchNormalization()(l2)
        l2 = layers.LeakyReLU(0.2)(l2)

        l2 = layers.Reshape((13, 2))(l2)
        l2 = upsample_block_v2(
            l2,
            20,
            layers.LeakyReLU(0.2),
            strides=3,
            kernel_size=12,
            # up_size=4,
            use_bias=True,
            use_bn=True,
            padding="same",
            use_dropout=False,
            name='l2_2'
        )
        l2 = upsample_block_v2(
            l2,
            10,
            layers.LeakyReLU(0.2),
            strides=2,
            kernel_size=12,
            # up_size=4,
            use_bias=True,
            use_bn=True,
            padding="same",
            use_dropout=False,
            name='l2_3'
        )
        l2 = upsample_block_v2(
            l2,
            5,
            layers.LeakyReLU(0.2),
            strides=1,
            kernel_size=12,
            # up_size=4,
            use_bias=True,
            use_bn=True,
            padding="same",
            use_dropout=False,
            name='l2_4'
        )
        l2 = upsample_block_v2(
            l2,
            1,
            layers.LeakyReLU(0.2),
            strides=2,
            kernel_size=12,
            # up_size=4,
            use_bias=True,
            use_bn=True,
            padding="same",
            use_dropout=False,
            name='l2_5'
        )

        l2 = layers.Flatten()(l2)
        l2 = layers.Activation('sigmoid')(l2)
        l2 = layers.Reshape(DATA_INPUT2)(l2)
        # At this point, we have an output which has the same shape as the input, (32, 32, 1).
        # We will use a Cropping2D layer to make it (28, 28, 1).
        # x = layers.Cropping1D(1)(x)

        g_model = tf.keras.models.Model(noise, (l1, l2), name="generator")
        return g_model


class DiscriminatorBaseIWGAN:
    @staticmethod
    def get_discriminator_model():
        data_input1 = layers.Input(shape=DATA_INPUT1)
        data_input2 = layers.Input(shape=DATA_INPUT2)

        # l1 = conv_block(
        #     data_input1,
        #     16,
        #     kernel_size=5,
        #     strides=2,
        #     use_bn=False,
        #     use_bias=True,
        #     activation=layers.LeakyReLU(0.2),
        #     use_dropout=False,
        #     drop_value=0.3,
        #     name='l1_conv1'
        # )
        # l1 = conv_block(
        #     l1,
        #     8,
        #     kernel_size=5,
        #     strides=2,
        #     use_bn=False,
        #     use_bias=True,
        #     activation=layers.LeakyReLU(0.2),
        #     use_dropout=False,
        #     drop_value=0.3,
        #     name='l1_conv2'
        # )

        l1 = layers.Dense(10, name='l1_dense1')(data_input1)
        l1 = layers.LeakyReLU(0.25)(l1)
        l1 = layers.Dense(30, name='l1_dense2', use_bias=True)(l1)
        l1 = layers.LeakyReLU(0.25)(l1)
        l1 = layers.Dense(8, name='l1_dense3', use_bias=True)(l1)
        # l1 = layers.Dense(10)(l1)

        l2 = conv_block(
            data_input2,
            32,
            kernel_size=12,
            strides=2,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
            name='l2_conv1'
        )
        l2 = conv_block(
            l2,
            16,
            kernel_size=12,
            strides=3,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
            name='l2_conv2'
        )
        l2 = conv_block(
            l2,
            8,
            kernel_size=12,
            strides=4,
            use_bn=False,
            use_bias=True,
            activation=layers.LeakyReLU(0.2),
            use_dropout=False,
            drop_value=0.3,
            name='l2_conv3'
        )

        x = layers.Concatenate(axis=1)([l1, l2])

        x = layers.Flatten()(x)
        x = layers.Dropout(0.3)(x)
        x = layers.Dense(1, name='dense1')(x)
        # x = layers.Activation('softmax')(x)

        d_model = tf.keras.models.Model([data_input1, data_input2], x, name="discriminator")
        return d_model
