from util.data_constants import col_cont, col_dummy, col_binary_dict, \
    col_drop, all_tags_after, desc_tags_expanded, col_names, GEN2, GEN
from sklearn.preprocessing import KBinsDiscretizer
from datetime import datetime
import pandas as pd
import os

# argus -r packet.pcap -w pck.argus

# ra -u -s stime saddr:39 daddr:39 dur sport dport proto:10 stos dtos sttl sbytes dbytes sload:15
# dload:15 srate drate synack ackdat spkts dpkts -r pck.argus > out.txt

# netstat -nb | sls -Context 0,1 spotify

SEC_IN_DAY = 86400


def encode_text_dummy(df, name):
    dummies = pd.get_dummies(df[name])
    for x in dummies.columns:
        dummy_name = f"{name}-{x}"
        df[dummy_name] = dummies[x]
    df.drop(name, axis=1, inplace=True)


def encode_text_binary(df, name, bits: int):
    b_arr = list(zip(*[[int(x) for x in list(f'{{0:0{str(bits)}b}}'.format(int(val), '08b'))] for val in df[name]]))
    for idx, arr in enumerate(b_arr):
        df[f"{name}_{idx}"] = arr
    df.drop(name, axis=1, inplace=True)


def encode_numeric_zscore(df, name, mean=None, sd=None):
    df[name] = (df[name] - mean) / sd


def encode_numeric_min_max(df, name, min=None, max=None):
    df[name] = (df[name] - min) / (max - min)


def prep_time(ts):
    dt = datetime.utcfromtimestamp(ts)
    df2 = dt - dt.replace(hour=0, minute=0, second=0, microsecond=0)
    return dt.isoweekday(), df2.total_seconds() / SEC_IN_DAY


def remove_special_chars(path):
    with open(path, 'r') as infile:
        data = infile.read()
        data = data.replace('*', '')
    with open(path, 'w') as outfile:
        outfile.write(data)


def prepare_recorded_data(path, _ip_list=None):
    remove_special_chars(path)

    # prepare timestamp
    df = pd.read_fwf(path)
    df.dropna(inplace=True)
    df.replace({'*': ''})
    df['weekday'], df['daytime'] = list(zip(*list(map(prep_time, df['StartTime']))))

    # binarize data
    for name, bits in col_binary_dict.items():
        encode_text_binary(df, name, bits)

    # normalize cont data
    cont_tags_map = list(map(lambda _name: (_name, df[_name].mean(), df[_name].std()), col_cont))
    [encode_numeric_zscore(df, name, _min, _max) for (name, _min, _max) in cont_tags_map]

    # dummy data
    [encode_text_dummy(df, name) for name in col_dummy]

    # prepare ports data
    df['Sport'] = ['other' if val not in ['domain', 'https'] else val for val in df['Sport']]
    df['Dport'] = ['other' if val not in ['domain', 'https'] else val for val in df['Dport']]
    [encode_text_dummy(df, name) for name in ['Sport', 'Dport']]

    # select all rows of specyfic flow
    if _ip_list is not None:
        # df = df[(df['DstAddr'] == '192.168.0.234') | (df['SrcAddr'] == '192.168.0.234')]
        # df[lambda row: tuple(item == row['DstAddr'] for item in _ip_list).any()]
        df = df[lambda row: (row['DstAddr'].isin(_ip_list)) | (row['SrcAddr'].isin(_ip_list))]

    # drop unnecessary columns
    [df.drop(val, axis=1, inplace=True) for val in col_drop]

    df.to_csv(os.path.join(os.path.dirname(path), f'{os.path.splitext(os.path.basename(path))[0]}_prepared.csv'),
              index=False)
    print('Done')


def prepare_recorded_data_as_cont(path, _ip_list=None):
    remove_special_chars(path)

    # prepare timestamp
    df = pd.read_fwf(path)
    df.dropna(inplace=True)
    df.replace({'*': ''})
    df['weekday'], df['daytime'] = list(zip(*list(map(prep_time, df['StartTime']))))

    # normalize cont data
    cont_tags_map = list(
        map(lambda _name: (_name, df[_name].min(), df[_name].max()), col_cont + list(col_binary_dict.keys())))
    [encode_numeric_min_max(df, name, _min, _max) for (name, _min, _max) in cont_tags_map]

    # select all rows of specyfic flow
    if _ip_list is not None:
        # df = df[(df['DstAddr'] == '192.168.0.234') | (df['SrcAddr'] == '192.168.0.234')]
        # df[lambda row: tuple(item == row['DstAddr'] for item in _ip_list).any()]
        df = df[lambda row: (row['DstAddr'].isin(_ip_list)) | (row['SrcAddr'].isin(_ip_list))]

    # drop unnecessary columns
    [df.drop(val, axis=1, inplace=True) for val in list(filter(lambda x: x != 'Proto--icmp', col_drop))]
    [df.drop(val, axis=1, inplace=True) for val in ['Sport', 'Dport', 'Proto', 'weekday']]

    df.to_csv(os.path.join(os.path.dirname(path), f'{os.path.splitext(os.path.basename(path))[0]}_cont_prepared_v1.csv'),
              index=False)
    print('Done')


def prepare_recorded_data_gen2(path, _ip_list=None):
    remove_special_chars(path)

    # prepare timestamp
    df = pd.read_fwf(path)
    df.dropna(inplace=True)
    df.replace({'*': ''})
    df['weekday'], df['daytime'] = list(zip(*list(map(prep_time, df['StartTime']))))

    # binarize data
    for name, bits in GEN2.col_binary_dict.items():
        encode_text_binary(df, name, bits)

    # normalize cont data
    cont_tags_map = list(map(lambda _name: (_name, df[_name].min(), df[_name].max()), GEN2.col_cont))
    [encode_numeric_min_max(df, name, _min, _max) for (name, _min, _max) in cont_tags_map]

    [encode_text_dummy(df, name) for name in GEN2.col_dummy]

    # prepare ports data
    df['Sport'] = ['other' if val not in ['domain', 'https'] else val for val in df['Sport']]
    df['Dport'] = ['other' if val not in ['domain', 'https'] else val for val in df['Dport']]
    [encode_text_dummy(df, name) for name in ['Sport', 'Dport']]

    # select all rows of specyfic flow
    if _ip_list is not None:
        df = df[lambda row: (row['DstAddr'].isin(_ip_list)) | (row['SrcAddr'].isin(_ip_list))]

    # drop unnecessary columns
    [df.drop(val, axis=1, inplace=True) for val in GEN2.col_drop]

    df.to_csv(os.path.join(os.path.dirname(path),
                           f'{os.path.splitext(os.path.basename(path))[0]}_prepared_GEN2.csv'),
              index=False)
    print('Done')


def discrete(value):
    return 1 if value >= 0.5 else 0


def quantify_data(path, save=True, gen2=False):
    df = pd.read_csv(path)
    df.columns = GEN2.all_tags_after if gen2 else all_tags_after
    dte = GEN2.desc_tags_expanded if gen2 else desc_tags_expanded
    for tag in dte:
        df[tag] = df[tag].apply(discrete)
    if save:
        df.to_csv(os.path.join(os.path.dirname(path), f'{os.path.splitext(os.path.basename(path))[0]}_desc.csv'),
                  index=False)
    return df


def add_labels(path, cols, save=True):
    df = pd.read_csv(path)
    df.columns = cols
    if save:
        df.to_csv(os.path.join(os.path.dirname(path), f'{os.path.splitext(os.path.basename(path))[0]}_cols.csv'),
                  index=False)
    return df


def create_data_mix(path_normal, path_anomaly):
    PATH = os.path.dirname(__file__)
    df_normal = pd.read_csv(os.path.join(PATH, path_normal))
    df_anomaly = pd.read_csv(os.path.join(PATH, path_anomaly))

    df_anomaly['class'] = 1
    df_normal['class'] = 0

    df_mixed = df_normal.append(df_anomaly).sample(frac=1, random_state=42)

    # df_mixed = df_mixed.drop('weekday', axis=1)

    df_mixed.to_csv(os.path.join(os.path.dirname(path_normal),
                                 f'{os.path.splitext(os.path.basename(path_normal))[0]}_mixed.csv'),
                    index=False)
    print('done')


if __name__ == '__main__':
    ip_list = ['2a00:1450:401b:80d::200e', '2a00:1450:401b:80e::2016', '2a03:2880:f016:19:face:b00c:0:2825',
               '35.186.224.47', '20.199.120.151', '198.252.206.25']

    # prepare_recorded_data('..\\saved_data\\custom_data\\p2_spotify.txt', ip_list)
    # prepare_recorded_data_gen2('..\\saved_data\\custom_data\\p3_raw.csv')
    # prepare_recorded_data_as_cont('..\\saved_data\\custom_data\\p3_raw.csv')

    create_data_mix('..\\saved_data\\custom_data2\\p2_spotify_gen3_prepared.csv',
                    f'..\\saved_data\\wgan_{GEN}_data_generated_1_cols_decoded_cont.csv')

    # quantify_data('..\\saved_data\\wgan_gp_data_generated_02.csv')
